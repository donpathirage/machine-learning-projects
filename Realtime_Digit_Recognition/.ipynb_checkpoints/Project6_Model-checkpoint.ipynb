{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import pickle\n",
    "import datetime\n",
    "import dateutil.tz\n",
    "import argparse\n",
    "import h5py\n",
    "import keras\n",
    "import sklearn\n",
    "import sklearn.preprocessing\n",
    "import tensorflow as tf\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Conv2D, MaxPooling2D, ZeroPadding2D, Dropout, Flatten, Activation\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.preprocessing import image\n",
    "from keras.constraints import maxnorm\n",
    "from keras.utils import np_utils\n",
    "from keras import backend as K\n",
    "from keras.datasets import mnist\n",
    "from keras import backend as K\n",
    "from keras import regularizers\n",
    "from keras import optimizers\n",
    "from keras.layers.core import Lambda\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.preprocessing import image\n",
    "from keras.applications.vgg16 import preprocess_input\n",
    "from keras import applications\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_SVHN(path, val_size):\n",
    "    with h5py.File(path+'/SVHN_train.hdf5', 'r') as f:\n",
    "        shape = f[\"X\"].shape\n",
    "        x_train = f[\"X\"][:shape[0]-val_size]\n",
    "        y_train = f[\"Y\"][:shape[0]-val_size].flatten()\n",
    "        x_val = f[\"X\"][shape[0]-val_size:]\n",
    "        y_val = f[\"Y\"][shape[0] - val_size:].flatten()\n",
    "\n",
    "    with h5py.File(path+'/SVHN_test.hdf5', 'r') as f:\n",
    "        x_test = f[\"X\"][:]\n",
    "        y_test = f[\"Y\"][:].flatten()\n",
    "\n",
    "    y_train = keras.utils.to_categorical(y_train, 10)\n",
    "    y_val = keras.utils.to_categorical(y_val, 10)\n",
    "    y_test = keras.utils.to_categorical(y_test, 10)\n",
    "\n",
    "    return (x_train, y_train), (x_val, y_val), (x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############## Hyperparameters #############\n",
    "\n",
    "batch_size = 10\n",
    "learning_rate = 0.1\n",
    "weight_decay = 0.0005\n",
    "lr_decay = 1e-6\n",
    "lr_drop = 20\n",
    "maxepochs = 200\n",
    "\n",
    "################ Optimizers ###############\n",
    "\n",
    "#optim = optimizers.SGD(lr=learning_rate, decay=lr_decay, momentum=0.9, nesterov=True)\n",
    "\n",
    "optim = optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=lr_decay, amsgrad=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################# MNIST Dataset ##############\n",
    "\n",
    "# (x_train, y_train), (x_test, y_test) = mnist.load_data() \n",
    "\n",
    "# img_size = 64\n",
    "\n",
    "# def upsample(img):\n",
    "#     img = cv2.resize(img,(img_size,img_size),interpolation = cv2.INTER_AREA)\n",
    "#     return img\n",
    "\n",
    "# def upsampleData(img_size, data, filename):\n",
    "    \n",
    "#     new_imgs = []\n",
    "#     new_img = []\n",
    "\n",
    "#     for i in range(round(len(data))):\n",
    "#         new_img = np.dstack((upsample(data[i]),upsample(data[i]),upsample(data[i])))\n",
    "#         new_imgs.append(new_img)\n",
    "    \n",
    "#     new_imgs=np.array(new_imgs).reshape(-1,img_size,img_size,3)\n",
    "    \n",
    "#     pickle_out = open(filename, \"wb\")             #Save bitstream to external file\n",
    "#     pickle.dump(new_imgs, pickle_out)\n",
    "#     pickle_out.close()\n",
    "    \n",
    "#     return new_imgs\n",
    "\n",
    "# try:\n",
    "#     pickle_in = open(\"MNIST_x_train\", \"rb\")          #If upsampled data already exists just load it\n",
    "#     x_train = pickle.load(pickle_in)\n",
    "\n",
    "#     pickle_in = open(\"MNIST_x_test\", \"rb\")\n",
    "#     x_test = pickle.load(pickle_in)\n",
    "#     print('Upsampled MNIST dataset loaded')\n",
    "    \n",
    "# except:\n",
    "#     print('Upsampled dataset not found, building new dataset')      #Otherwise create new data\n",
    "#     x_train = upsampleData(img_size, x_train, filename = 'MNIST_x_train' )\n",
    "#     x_test = upsampleData(img_size, x_test, filename = 'MNIST_x_test' )\n",
    "#     print('Upsampled MNIST dataset loaded')\n",
    "\n",
    "# num_classes = 10\n",
    "# x_shape = [img_size,img_size,3]\n",
    "# y_train = np_utils.to_categorical(y_train, num_classes)\n",
    "# y_test = np_utils.to_categorical(y_test, num_classes)\n",
    "# x_train = x_train.astype('float32')\n",
    "# x_test = x_test.astype('float32')\n",
    "# x_train  /= 255\n",
    "# x_test /= 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.imshow(x_train[2])\n",
    "plt.title('class: ' + str(y_train[2]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.imshow(x_test[1])\n",
    "plt.title('class: ' + str(y_train[2]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datagen = ImageDataGenerator(\n",
    "    \n",
    "    featurewise_center=False,  # set input mean to 0 over the dataset\n",
    "    samplewise_center=False,  # set each sample mean to 0\n",
    "    featurewise_std_normalization=False,  # divide inputs by std of the dataset\n",
    "    samplewise_std_normalization=False,  # divide each input by its std\n",
    "    zca_whitening=False,  # apply ZCA whitening\n",
    "    rotation_range=15,  # randomly rotate images in the range (degrees, 0 to 180)\n",
    "    width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)\n",
    "    height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)\n",
    "    horizontal_flip=True,  # randomly flip images\n",
    "    vertical_flip=False)  # randomly flip images\n",
    "# (std, mean, and principal components if ZCA whitening is applied).\n",
    "\n",
    "def lr_scheduler(epoch):\n",
    "            return learning_rate * (0.5 ** (epoch // lr_drop))\n",
    "    \n",
    "reduce_lr = keras.callbacks.LearningRateScheduler(lr_scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MNIST_model(weights_path=None):\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32, (3, 3), padding='same', input_shape=x_shape, kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.3))\n",
    "\n",
    "    model.add(Conv2D(32, (3, 3), padding='same',kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(BatchNormalization())\n",
    "\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "    model.add(Conv2D(64, (3, 3), padding='same',kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.4))\n",
    "\n",
    "    model.add(Conv2D(64, (3, 3), padding='same',kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(BatchNormalization())\n",
    "\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "    model.add(Conv2D(128, (3, 3), padding='same',kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.4))\n",
    "\n",
    "    model.add(Conv2D(128, (3, 3), padding='same',kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.4))\n",
    "\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(512,kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(BatchNormalization())\n",
    "\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(10))\n",
    "    model.add(Activation('softmax'))    \n",
    "    \n",
    "    if weights_path:\n",
    "        model.load_weights(weights_path, by_name=True)\n",
    "    \n",
    "    model.compile(loss='categorical_crossentropy', optimizer=optim, metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MNIST_model = MNIST_model()\n",
    "MNIST_model.compile(loss='categorical_crossentropy', optimizer=optim, metrics=['accuracy'])\n",
    "MNIST_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn = MNIST_model.fit_generator(datagen.flow(x_train, y_train,\n",
    "                                               batch_size=batch_size),\n",
    "                                  \n",
    "                                steps_per_epoch=x_train.shape[0] // batch_size,\n",
    "                                epochs=maxepochs,\n",
    "                                validation_data=(x_test, y_test),callbacks=[reduce_lr], verbose=1, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Buster\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "new_model = tf.keras.models.load_model('mnist_segmentation.model')   #Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = new_model.predict(x_test[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
